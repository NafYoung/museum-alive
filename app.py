import streamlit as st
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Page Config
st.set_page_config(page_title="Museum Alive", page_icon="ğŸ›ï¸")

# Title and Description
st.title("ğŸ›ï¸ Museum Alive: Let Artifacts Speak")
st.write("Upload a photo of an artifact, and AI will bring it to life.")

# Sidebar for Settings
with st.sidebar:
    st.header("Settings")
    api_key = os.getenv("DEEPSEEK_API_KEY")
    if not api_key or "your-key-here" in api_key:
        st.warning("âš ï¸ Please set your DEEPSEEK_API_KEY in the .env file.")
    else:
        st.success("âœ… API Key Loaded")

import asyncio
import edge_tts
from openai import OpenAI

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from PIL import Image

# Initialize DeepSeek Client
client = OpenAI(
    api_key=os.getenv("DEEPSEEK_API_KEY"),
    base_url="https://api.deepseek.com"
)

# Sidebar Settings
with st.sidebar:
    st.header("Settings")
    api_key = os.getenv("DEEPSEEK_API_KEY")
    if not api_key:
        st.warning("âš ï¸ Please set DEEPSEEK_API_KEY in .env")
    else:
        st.success("âœ… API Key Loaded")
    
    # Vision Toggle (Default OFF to save Cloud resources)
    use_vision = st.toggle("Enable AI Vision (Experimental)", value=False, help="Turn this on ONLY if running locally. Streamlit Cloud may crash due to memory limits.")

# Initialize Vision Model (Lazy Load)
@st.cache_resource
def load_vision_model():
    model_id = "vikhyatk/moondream2"
    revision = "2024-04-02"
    model = AutoModelForCausalLM.from_pretrained(
        model_id, trust_remote_code=True, revision=revision
    )
    tokenizer = AutoTokenizer.from_pretrained(model_id, revision=revision)
    return model, tokenizer

# Main Content
st.write("Upload a photo of an artifact, and AI will bring it to life.")
uploaded_file = st.file_uploader("ğŸ“¸ ç»™ä»–æ‹å¼ ç…§ (æˆ–ä¸Šä¼ å›¾ç‰‡)", type=["jpg", "png", "jpeg"])

if uploaded_file is not None:
    st.image(uploaded_file, caption="å·²ä¸Šä¼ æ–‡ç‰©", use_container_width=True)
    
    artifact_description = ""
    
    # Logic Branch: Vision vs Manual
    if use_vision:
        if st.button("ğŸ‘ï¸ AI è¯†å›¾å¹¶è¯´è¯"):
            with st.spinner("æ­£åœ¨åŠ è½½è§†è§‰æ¨¡å‹ (ç¬¬ä¸€æ¬¡å¯èƒ½å¾ˆæ…¢)..."):
                try:
                    vision_model, vision_tokenizer = load_vision_model()
                    image = Image.open(uploaded_file)
                    with st.spinner("AI æ­£åœ¨è§‚å¯Ÿæ–‡ç‰©..."):
                        enc_image = vision_model.encode_image(image)
                        artifact_description = vision_model.answer_question(enc_image, "Describe this artifact in detail.", vision_tokenizer)
                        st.info(f"ğŸ‘€ AI çœ‹åˆ°çš„ï¼š{artifact_description}")
                except Exception as e:
                    st.error(f"è§†è§‰æ¨¡å‹åŠ è½½å¤±è´¥ (å¯èƒ½æ˜¯å†…å­˜ä¸è¶³): {e}")
                    st.stop()
    else:
        artifact_description = st.text_input("ğŸ’¡ (çœæµç‰ˆ) å‘Šè¯‰æˆ‘å®ƒçš„åå­—/ç‰¹å¾ï¼š", placeholder="æ¯”å¦‚ï¼šä¸‰æ˜Ÿå †é’é“œé¢å…·")
        if artifact_description and st.button("è®©å®ƒè¯´è¯ ğŸ—£ï¸"):
             pass # Trigger next block

    # Generate Story & Audio (Common Logic)
    if artifact_description:
         with st.spinner("æ­£åœ¨å”¤é†’æ²‰ç¡çš„çµé­‚..."):
            # 2. Generate Story
            story = get_artifact_story(artifact_description)
            st.markdown(f"### ğŸ“œ æ–‡ç‰©çš„è‡ªè¿°")
            st.write(story)
            
            # 3. Generate Audio
            output_file = "artifact_voice.mp3"
            asyncio.run(generate_audio(story, output_file))
            
            # 4. Play Audio
            st.audio(output_file)
            if not use_vision:
                st.success("ğŸ‰ å”¤é†’æˆåŠŸï¼(è¿™æ˜¯çœæµç‰ˆï¼Œæœ¬åœ°å¼€å¯ Vision å¯ä½“éªŒå…¨è‡ªåŠ¨)")
