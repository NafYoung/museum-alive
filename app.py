import streamlit as st
import os
from dotenv import load_dotenv
import asyncio
import edge_tts
from openai import OpenAI

# Load environment variables
load_dotenv()

# Page Config
st.set_page_config(page_title="Museum Alive", page_icon="ğŸ›ï¸")
st.title("ğŸ›ï¸ Museum Alive: Let Artifacts Speak")

# --- SAFE IMPORT SECTION ---
# Streamlit Cloud free tier might crash on `import torch`. 
# We wrap this to let the app load even if libs are missing/crashing.
try:
    import torch
    from transformers import AutoModelForCausalLM, AutoTokenizer
    from PIL import Image
    VISION_AVAILABLE = True
except ImportError:
    VISION_AVAILABLE = False
    st.error("âš ï¸ AI Vision libraries failed to load. Falling back to Text-Only mode.")
except Exception as e:
    VISION_AVAILABLE = False
    # st.warning(f"Note: Vision features disabled due to load error: {e}")

# Initialize DeepSeek Client
client = OpenAI(
    api_key=os.getenv("DEEPSEEK_API_KEY"),
    base_url="https://api.deepseek.com"
)

# Sidebar Settings
with st.sidebar:
    st.header("Settings")
    api_key = os.getenv("DEEPSEEK_API_KEY")
    if not api_key:
        st.warning("âš ï¸ Please set DEEPSEEK_API_KEY in .env")
    else:
        st.success("âœ… API Key Loaded")
    
    # Vision Toggle
    if VISION_AVAILABLE:
        use_vision = st.toggle("Enable AI Vision (Experimental)", value=False, help="Turn this on ONLY if running locally.")
    else:
        use_vision = False
        st.caption("ğŸš« Vision unavailable (Libs missing)")

# Initialize Vision Model (Lazy Load)
@st.cache_resource
def load_vision_model():
    if not VISION_AVAILABLE:
        return None, None
        
    model_id = "vikhyatk/moondream2"
    revision = "2024-04-02"
    
    # 1. Load Tokenizer FIRST
    tokenizer = AutoTokenizer.from_pretrained(model_id, revision=revision)
    
    # 2. Load Model
    model = AutoModelForCausalLM.from_pretrained(
        model_id, trust_remote_code=True, revision=revision
    )
    
    # 3. Apply Patch
    if not hasattr(model.config, 'pad_token_id'):
        model.config.pad_token_id = tokenizer.pad_token_id
        
    return model, tokenizer

async def generate_audio(text, output_file="output.mp3"):
    """Generate audio using Edge-TTS (Free)"""
    communicate = edge_tts.Communicate(text, "zh-CN-YunxiNeural")
    await communicate.save(output_file)

def get_artifact_story(artifact_description):
    """Ask DeepSeek to roleplay based on visual description"""
    prompt = f"""
    æˆ‘ç»™ä½ çœ‹äº†ä¸€å¼ æ–‡ç‰©çš„å›¾ç‰‡ï¼Œå®ƒçš„ç‰¹å¾æ˜¯ï¼š{artifact_description}ã€‚
    
    è¯·ä½ æ ¹æ®è¿™ä¸ªæè¿°ï¼ŒçŒœçŒœä½ å¯èƒ½æ˜¯è°ï¼ˆå¦‚æœç‰¹å¾å¾ˆæ˜æ˜¾ï¼‰ï¼Œæˆ–è€…å°±ä½œä¸ºä¸€ä¸ªç¥ç§˜çš„å¤ç‰©ã€‚
    
    è¯·ç”¨ç¬¬ä¸€äººç§°ï¼ˆâ€œæˆ‘â€ï¼‰åšä¸€ä¸ªè‡ªæˆ‘ä»‹ç»ã€‚
    
    è¦æ±‚ï¼š
    1. æ—¢ç„¶æ˜¯â€œè®©æ–‡ç‰©è¯´è¯â€ï¼Œè¯­æ°”è¦ç¬¦åˆä½ çš„èº«ä»½ã€‚
    2. ä¸è¦åªè®²æ¯ç‡¥çš„æ•°æ®ï¼Œè¦è®²ä½ çš„æ„Ÿå—ã€‚
    3. ç¯‡å¹…æ§åˆ¶åœ¨ 150 å­—ä»¥å†…ã€‚
    4. å¼€å¤´è¦å¸å¼•äººã€‚
    """
    
    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªåšç‰©é¦†é‡Œçš„æ–‡ç‰©ï¼Œå¯Œæœ‰æ€§æ ¼å’Œæƒ…æ„Ÿã€‚"},
                {"role": "user", "content": prompt},
            ],
            stream=False
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"å“å‘€ï¼Œæˆ‘çœ‹ä¸æ¸…è‡ªå·±... ({str(e)})"

# Main Content
st.write("Upload a photo of an artifact, and AI will bring it to life.")
uploaded_file = st.file_uploader("ğŸ“¸ ç»™ä»–æ‹å¼ ç…§ (æˆ–ä¸Šä¼ å›¾ç‰‡)", type=["jpg", "png", "jpeg"])

if uploaded_file is not None:
    st.image(uploaded_file, caption="å·²ä¸Šä¼ æ–‡ç‰©", use_container_width=True)
    
    artifact_description = ""
    
    # Logic Branch: Vision vs Manual
    if use_vision:
        if st.button("ğŸ‘ï¸ AI è¯†å›¾å¹¶è¯´è¯"):
            with st.spinner("æ­£åœ¨åŠ è½½è§†è§‰æ¨¡å‹ (ç¬¬ä¸€æ¬¡å¯èƒ½å¾ˆæ…¢)..."):
                try:
                    vision_model, vision_tokenizer = load_vision_model()
                    image = Image.open(uploaded_file)
                    with st.spinner("AI æ­£åœ¨è§‚å¯Ÿæ–‡ç‰©..."):
                        enc_image = vision_model.encode_image(image)
                        artifact_description = vision_model.answer_question(enc_image, "Describe this artifact in detail.", vision_tokenizer)
                        st.info(f"ğŸ‘€ AI çœ‹åˆ°çš„ï¼š{artifact_description}")
                except Exception as e:
                    st.error(f"è§†è§‰æ¨¡å‹åŠ è½½å¤±è´¥ (å¯èƒ½æ˜¯å†…å­˜ä¸è¶³): {e}")
                    st.stop()
    else:
        artifact_description = st.text_input("ğŸ’¡ (çœæµç‰ˆ) å‘Šè¯‰æˆ‘å®ƒçš„åå­—/ç‰¹å¾ï¼š", placeholder="æ¯”å¦‚ï¼šä¸‰æ˜Ÿå †é’é“œé¢å…·")
        if artifact_description and st.button("è®©å®ƒè¯´è¯ ğŸ—£ï¸"):
             pass # Trigger next block

    # Generate Story & Audio (Common Logic)
    if artifact_description:
         with st.spinner("æ­£åœ¨å”¤é†’æ²‰ç¡çš„çµé­‚..."):
            # 2. Generate Story
            story = get_artifact_story(artifact_description)
            st.markdown(f"### ğŸ“œ æ–‡ç‰©çš„è‡ªè¿°")
            st.write(story)
            
            # 3. Generate Audio
            output_file = "artifact_voice.mp3"
            asyncio.run(generate_audio(story, output_file))
            
            # 4. Play Audio
            st.audio(output_file)
            if not use_vision:
                st.success("ğŸ‰ å”¤é†’æˆåŠŸï¼(è¿™æ˜¯çœæµç‰ˆï¼Œæœ¬åœ°å¼€å¯ Vision å¯ä½“éªŒå…¨è‡ªåŠ¨)")
